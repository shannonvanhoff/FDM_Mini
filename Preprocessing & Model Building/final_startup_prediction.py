# -*- coding: utf-8 -*-
"""FInal Startup Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qfIYmLlzQFGq0oWXd3L_UQyabzR1HRyq

# **1. DATA Exploration**


>
"""

# Import libraries
## Basic libs
import pandas as pd
import numpy as np
import warnings
## Data Visualization
import seaborn as sns
import matplotlib.pyplot as plt

# Configure libraries
warnings.filterwarnings('ignore')
plt.rcParams['figure.figsize'] = (10, 10)
plt.style.use('seaborn')

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
import os

drive.mount('/content/drive')
# %cd /content/drive/MyDrive/FDMMiniProject/

startup_dataset = pd.read_csv("startup_data.csv")

startup_dataset.head(10)

startup_dataset.info()

"""Identifying Data Types"""

startup_dataset.columns

"""Identifying Numeric Data"""

numeric=['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']
startup_dataset_num=startup_dataset.select_dtypes(include=numeric)
startup_dataset_num.head(3)

"""Identifying Categorical Data"""

startup_dataset_cat = startup_dataset.select_dtypes(include = 'object')
startup_dataset_cat.head(3)

"""Analysing Data"""

describeNum = startup_dataset.describe(include =['float64', 'int64', 'float', 'int'])
describeNum

describeNumCat = startup_dataset.describe(include=["O"])
describeNumCat

#age_first_funding_year and age_last_funding_year have the same min,max data
#Is the maximum/minimum value still within the reasonable limit?
#Min/max that is too far from the mean/median may be an indication of data input error
#Is there a column with a significant difference between the mean and the median?
#Differences between mean/median indicate outlier or skewed distribution

"""Graphic Approach to the dataset

Correlation heatmap
"""

startup_dataset.corr()

features = ['age_first_funding_year','age_last_funding_year','age_first_milestone_year','age_last_milestone_year','relationships','funding_rounds','funding_total_usd','milestones','is_CA','is_NY','is_MA','is_TX','is_otherstate','is_software','is_web','is_mobile','is_enterprise','is_advertising','is_gamesvideo','is_ecommerce','is_biotech','is_consulting','is_othercategory','has_VC','has_angel','has_roundA','has_roundB','has_roundC','has_roundD','avg_participants','is_top500','status']

plt.figure(figsize=(30,20))
ax = sns.heatmap(data = startup_dataset[features].corr(),cmap='YlGnBu',annot=True)

bottom, top = ax.get_ylim()
ax.set_ylim(bottom + 0.5,top - 0.5)

"""Scatter Plot"""

fig, ax = plt.subplots()
_ = plt.scatter(x=startup_dataset['age_first_funding_year'], y=startup_dataset['age_last_funding_year'], edgecolors="#000000", linewidths=0.5)
_ = ax.set(xlabel="age_first_funding_year", ylabel="age_last_funding_year")

"""Dataset collection founded years"""

startup_dataset["founded_at"].apply(lambda x: '20:' + x[:2]).value_counts(normalize=False)

startup_dataset["founded_at"].apply(lambda x: '20:' + x[:2]).value_counts(normalize=True)

"""total 563 startups or 60% of startups established in 2001

How many Startup are acquired or closed?
"""

startup_dataset_acquired = startup_dataset[(startup_dataset["status"] == 'acquired')]
startup_dataset_acquired.shape

startup_dataset_closed = startup_dataset[(startup_dataset["status"] == 'closed')]
startup_dataset_closed.shape

Succeeded = (startup_dataset['status'] == 'acquired').sum()
#Summing up all the values of column status with a
#condition for succeded and similary for failed
Failed = (startup_dataset['status'] == 'closed').sum()
print(Succeeded)
print(Failed)
p = [Succeeded, Failed]
plt.pie(p, #giving array
  labels = ['Succeed', 'Failed'], #Correspndingly giving labels
  colors = ['#003f5c', '#ffa600'], # Corresponding colors
  explode = (0.0015, 0), #How much the gap should me there between the pies
  startangle = 0) #what start angle should be given
plt.axis('equal')
plt.show()

"""Which category has the largest number of startup"""

fig, ax = plt.subplots(figsize=(12,8))

_ = sns.countplot(x="category_code", hue="status", data=startup_dataset, palette="nipy_spectral",
              order=startup_dataset.category_code.value_counts().index)

_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
_ = ax.set(xlabel="Category", ylabel="No. of startup")
plt.legend(bbox_to_anchor=(0.945, 0.90))

"""Which category has the largest number Success Rate"""

data1 = startup_dataset[startup_dataset['status']=='acquired'].groupby(['category_code']).agg({'status':'count'}).reset_index()
data1.columns=['category_code','total_success']

data2 = startup_dataset[startup_dataset['status']=='closed'].groupby(['category_code']).agg({'status':'count'}).reset_index()
data2.columns=['category_code','total_closed']

data3=startup_dataset.groupby(['category_code']).agg({'status':'count'}).reset_index()
data3.columns=['category_code','total_startup']

data1= data1.merge(data2, on='category_code')
data1= data1.merge(data3, on='category_code')

data1['success_rate']= round((data1['total_success'] / data1['total_startup']) * 100,2)

most_succes_rate = data1.sort_values('success_rate', ascending=False)
most_succes_rate

fig, ax = plt.subplots(figsize=(10,7))
_ = sns.barplot(x="category_code", y="success_rate", data=most_succes_rate,
                palette="nipy_spectral", ax=ax)
_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
_ = ax.set(xlabel="Category", ylabel="Success Rate of Start Up")

"""Which category having most number of total funding"""

funding_sorted_category = pd.pivot_table(startup_dataset,
              index=['category_code'],
              values=['funding_total_usd'],
              aggfunc=['sum']
              ).reset_index()
funding_sorted_category.columns = ['category_code', 'funding_total_usd']
funding_sorted_category = funding_sorted_category.sort_values(['funding_total_usd'], ascending = False)
funding_sorted_category.head(10)

fig, ax = plt.subplots(figsize=(15,7))
_ = sns.barplot(x="category_code", y="funding_total_usd", data=funding_sorted_category,
                palette="nipy_spectral", ax=ax)
_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
_ = ax.set(xlabel="Category", ylabel="Total Funding USD")

"""Which State having most number of acquired Startup per category"""

trending_statea = startup_dataset_acquired.groupby(['state_code','category_code']).size().rename('num_startup').reset_index()

most_trending_statea = trending_statea[trending_statea.groupby('state_code')['num_startup'].transform(max) == trending_statea['num_startup']]
most_trending_statea = most_trending_statea.sort_values('num_startup', ascending=False)
most_trending_statea.head(10)

"""# **2. DATA Preporcessing**

>

Changing the data value of the 'status' Column and handling 'status' data type to int
"""

status = {'acquired':1, 'closed':0}

startup_dataset.status = [status[item] for item in startup_dataset.status]

startup_dataset['status'].astype(int)

startup_dataset.head()

"""Drop Labels Column"""

#Check similarity between labes column and status column
for index, row in startup_dataset.iterrows():
    if row['labels']!=row['status']:
        print(index, row['labels'], row['status'])

#drop feature
startup_dataset.drop(["labels"], axis=1, inplace=True)

null=pd.DataFrame(startup_dataset.isnull().sum(),columns=["Null Values"])
null["% Missing Values"]=(startup_dataset.isna().sum()/len(startup_dataset)*100)
null = null[null["% Missing Values"] > 0]
null

# Checking Missing Values Column 
startup_dataset[["Unnamed: 6", "closed_at", "age_first_milestone_year", "age_last_milestone_year", "state_code.1", "status"]].head(4)

"""Handling Missing Value Unnamed: 6"""

#Drop this column as we don't have enough information
startup_dataset.drop(["Unnamed: 6"],axis=1, inplace=True)

"""Handling 'state_code' and 'state_code.1' columns"""

#compare the 'state_code' and 'state_code.1' column and drop if it's similar
for index, row in startup_dataset.iterrows():
    if row['state_code']!=row['state_code.1']:
        print(index, row['state_code'], row['state_code.1'])

#Drop 'state_code.1' Column
startup_dataset.drop(["state_code.1"], axis=1, inplace=True)

print(startup_dataset.isnull().sum())

"""Handling Missing Value closed_at"""

#Convert closed_at values in a column into binary
#Notice that nan value mean the Startup is still operating. Moreover, with the none value we cant use it with time data analysis

startup_dataset['closed_at']=startup_dataset['closed_at'].fillna('temporary')
startup_dataset['closed_at'] = startup_dataset.closed_at.apply(lambda x: 1 if x =='temporary' else 0)

"""Handling Missing Value age_first_milestone_year and age_last_milestone_year"""

startup_dataset[['age_first_milestone_year','age_last_milestone_year','milestones']].head()

startup_dataset['age_first_milestone_year'] = startup_dataset['age_first_milestone_year'].fillna(value="0")
startup_dataset['age_last_milestone_year'] = startup_dataset['age_last_milestone_year'].fillna(value="0")

print(startup_dataset.isnull().sum())

"""Check Duplicate Values"""

duplicate = startup_dataset[startup_dataset.duplicated()] 
  
print("Duplicate Rows :")

"""Check Negative Values"""

startup_dataset['age_first_milestone_year'] = startup_dataset.age_first_milestone_year.astype(float)
startup_dataset['age_last_milestone_year'] = startup_dataset.age_last_milestone_year.astype(float)

age=["age_first_funding_year","age_last_funding_year","age_first_milestone_year","age_last_milestone_year"]

for a in range(len(age)):
    print("Is there any negative value in '{}' column  : {} ".format(age[a],(startup_dataset[age[a]]<0).any()))

#Drop Negative Values
startup_dataset=startup_dataset.drop(startup_dataset[startup_dataset.age_first_funding_year<0].index)
startup_dataset=startup_dataset.drop(startup_dataset[startup_dataset.age_last_funding_year<0].index)
startup_dataset=startup_dataset.drop(startup_dataset[startup_dataset.age_first_milestone_year<0].index)
startup_dataset=startup_dataset.drop(startup_dataset[startup_dataset.age_last_milestone_year<0].index)

for a in range(len(age)):
    print("Is there any negative value in '{}' column  : {} ".format(age[a],(startup_dataset[age[a]]<0).any()))

"""Check Outliers"""

featuresNumfinal = ['age_first_funding_year','age_last_funding_year','age_first_milestone_year','age_last_milestone_year','funding_total_usd']

plt.figure(figsize=(15, 7))
for i in range(0, len(featuresNumfinal)):
    plt.subplot(1, len(featuresNumfinal), i+1)
    sns.boxplot(y=startup_dataset[featuresNumfinal[i]], color='green', orient='v')
    plt.tight_layout()

"""Remove Outliers"""

startup_dataset["age_first_funding_year"] = np.log1p(startup_dataset["age_first_funding_year"])
startup_dataset["age_last_funding_year"] = np.log1p(startup_dataset["age_last_funding_year"])
startup_dataset["age_first_milestone_year"] = np.log1p(startup_dataset["age_first_milestone_year"])
startup_dataset["age_last_milestone_year"] = np.log1p(startup_dataset["age_last_milestone_year"])
startup_dataset["funding_total_usd"] = np.log1p(startup_dataset["funding_total_usd"])

featuresNumfinal = ['age_first_funding_year','age_last_funding_year','age_first_milestone_year','age_last_milestone_year','funding_total_usd']

plt.figure(figsize=(15, 7))
for i in range(0, len(featuresNumfinal)):
    plt.subplot(1, len(featuresNumfinal), i+1)
    sns.boxplot(y=startup_dataset[featuresNumfinal[i]], color='green', orient='v')
    plt.tight_layout()

startup_dataset.info()

"""Remove unwanted Data columns"""

startup_dataset.drop(["last_funding_at"],axis=1, inplace=True)
startup_dataset.drop(["first_funding_at"], axis=1, inplace=True)
startup_dataset.drop(["founded_at"], axis=1, inplace=True)
startup_dataset.drop(["id"], axis=1, inplace=True)
startup_dataset.drop(["closed_at"], axis=1, inplace=True)
startup_dataset.drop(["object_id"], axis=1, inplace=True)
startup_dataset.drop(["zip_code"], axis=1, inplace=True)
startup_dataset.drop(["name"], axis=1, inplace=True)
startup_dataset.drop(["has_roundB"], axis=1, inplace=True)
startup_dataset.drop(["age_first_milestone_year"], axis=1, inplace=True)
startup_dataset.drop(["age_last_milestone_year"], axis=1, inplace=True)
startup_dataset.drop(["Unnamed: 0"], axis=1, inplace=True)
startup_dataset.drop(["latitude"], axis=1, inplace=True)
startup_dataset.drop(["longitude"], axis=1, inplace=True)
startup_dataset.drop(["age_first_funding_year"], axis=1, inplace=True)
startup_dataset.drop(["age_last_funding_year"], axis=1, inplace=True)
startup_dataset.drop(["is_CA"], axis=1, inplace=True)
startup_dataset.drop(["is_NY"], axis=1, inplace=True)
startup_dataset.drop(["is_MA"], axis=1, inplace=True)
startup_dataset.drop(["is_TX"], axis=1, inplace=True)
startup_dataset.drop(["is_otherstate"], axis=1, inplace=True)
startup_dataset.drop(["is_software"], axis=1, inplace=True)
startup_dataset.drop(["is_web"], axis=1, inplace=True)
startup_dataset.drop(["is_mobile"], axis=1, inplace=True)
startup_dataset.drop(["is_enterprise"], axis=1, inplace=True)
startup_dataset.drop(["is_advertising"], axis=1, inplace=True)
startup_dataset.drop(["is_gamesvideo"], axis=1, inplace=True)
startup_dataset.drop(["is_ecommerce"], axis=1, inplace=True)
startup_dataset.drop(["is_biotech"], axis=1, inplace=True)
startup_dataset.drop(["is_consulting"], axis=1, inplace=True)
startup_dataset.drop(["is_othercategory"], axis=1, inplace=True)
startup_dataset.drop(["has_VC"], axis=1, inplace=True)
startup_dataset.drop(["has_angel"], axis=1, inplace=True)
startup_dataset.drop(["has_roundA"], axis=1, inplace=True)
startup_dataset.drop(["has_roundC"], axis=1, inplace=True)
startup_dataset.drop(["has_roundD"], axis=1, inplace=True)
startup_dataset.drop(["avg_participants"], axis=1, inplace=True)
startup_dataset.drop(["city"], axis=1, inplace=True)

startup_dataset.info()

startup_dataset['state_code'].unique()

startup_dataset['category_code'].unique()

# Import label encoder
from sklearn import preprocessing
  
# label_encoder object knows how to understand word labels.
label_encoder = preprocessing.LabelEncoder()
  
# Encode labels in column 'state_code'.
startup_dataset['state_code']= label_encoder.fit_transform(startup_dataset['state_code'])
  
startup_dataset['state_code'].unique()

# Encode labels in column 'category_code'.
startup_dataset['category_code']= label_encoder.fit_transform(startup_dataset['category_code'])
  
startup_dataset['category_code'].unique()

startup_dataset.head()

Startup_Dataframe=pd.DataFrame(startup_dataset)

Startup_Dataframe

Startup_Dataframe.to_csv("cleaned_dataset.csv", index="False")

"""# **3. DATA Modeling**

> 


"""

pip install scikit-plot

#Models import
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve
#import imputer:
from sklearn.impute import KNNImputer
#score
from sklearn.metrics import f1_score
from sklearn.ensemble import StackingClassifier

from sklearn.model_selection import train_test_split

from sklearn.metrics import confusion_matrix, classification_report

import scikitplot as skplot

y=Startup_Dataframe["status"]
X= Startup_Dataframe.loc[:, Startup_Dataframe.columns != 'status']
X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2, random_state=42)

# Cross validate model with Kfold stratified cross val
kfold = StratifiedKFold(n_splits=10)

# Modeling step Test differents algorithms 
random_state = 2
classifiers = []
classifiers.append(SVC(random_state=random_state))
classifiers.append(DecisionTreeClassifier(random_state=random_state))
classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))
classifiers.append(RandomForestClassifier(random_state=random_state))
classifiers.append(ExtraTreesClassifier(random_state=random_state))
classifiers.append(GradientBoostingClassifier(random_state=random_state))
classifiers.append(KNeighborsClassifier())
classifiers.append(LogisticRegression(random_state = random_state))
classifiers.append(LinearDiscriminantAnalysis())

print(classifiers[1])
print(cross_val_score(DecisionTreeClassifier(), X_train, y_train,cv = 3))

cv_results = []
for classifier in classifiers :
    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = "accuracy", cv = kfold, n_jobs=-1))

cv_means = []
cv_std = []
for cv_result in cv_results:
    cv_means.append(cv_result.mean())
    cv_std.append(cv_result.std())

cv_res = pd.DataFrame({"CrossValMeans":cv_means,"CrossValerrors": cv_std,"Algorithm":["SVC","DecisionTree","AdaBoost",
"RandomForest","ExtraTrees","GradientBoosting","KNeighboors","LogisticRegression","LinearDiscriminantAnalysis"]})

g = sns.barplot("CrossValMeans","Algorithm",data = cv_res, palette="Set3",orient = "h",**{'xerr':cv_std})
g.set_xlabel("Mean Accuracy")
g = g.set_title("Cross validation scores")
plt.show()

best_classifiers=[]

# RFC Parameters tunning 
RFC = RandomForestClassifier()


## Search grid for optimal parameters
rf_param_grid = {"max_depth": [1,2,4],
              "max_features": [1, 10,100,1000],
              "min_samples_split": [2, 3, 10],
              "min_samples_leaf": [1, 3, 10],
              "bootstrap": [False],
              "n_estimators" :[100,300],
              "criterion": ["gini"]}

gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring="accuracy", n_jobs= -1, verbose = 1)

gsRFC.fit(X_train,y_train)

RFC_best = gsRFC.best_estimator_

best_classifiers.append(RFC_best)

# Best score

RandomForests_score= gsRFC.best_score_

print(f' RandomForests score is :{RandomForests_score}')

y_pred_rf = gsRFC.predict(X_test)

print(classification_report(y_test,y_pred_rf))

skplot.metrics.plot_confusion_matrix(y_test,y_pred_rf)

### LOGISTIC REGRESSION

log = LogisticRegression()
lg_param_grid = {'penalty' : ['l1','l2'], 
    'C'       : np.logspace(-3,3,7),
    'solver'  : ['newton-cg', 'lbfgs', 'liblinear']}
    
gsLR = GridSearchCV(log, param_grid = lg_param_grid, cv=kfold, scoring="accuracy", n_jobs= -1, verbose = 1)


gsLR.fit(X_train, y_train)

#y_pred_log = log.predict(X_test)
#print(classification_report(y_test,y_pred_log))

#skplot.metrics.plot_confusion_matrix(y_test,y_pred_log)
LR_best = gsLR.best_estimator_
best_classifiers.append(LR_best)

# Best score

LogisticRegression_score= gsLR.best_score_

print(f' LogisticRegression score is :{LogisticRegression_score}')

y_pred_lr = gsLR.predict(X_test)

print(classification_report(y_test,y_pred_lr))

skplot.metrics.plot_confusion_matrix(y_test,y_pred_lr)

# Gradient boosting tunning

GBC = GradientBoostingClassifier()
gb_param_grid = {'loss' : ["deviance"],
              'n_estimators' : [100,200,1000],
              'learning_rate': [0.001,0.1, 0.05, 0.01,1,10],
              'max_depth': [4, 8],
              'min_samples_leaf': [100,150],
              'max_features': [0.3, 0.1] 
              }

gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring="accuracy", n_jobs= -1, verbose = 1)

gsGBC.fit(X_train,y_train)

GBC_best = gsGBC.best_estimator_
best_classifiers.append(GBC_best)

# Best score

GradientBoosting_score= gsGBC.best_score_

print(f' GradientBoosting score is :{GradientBoosting_score}')

y_pred_gbc = gsGBC.predict(X_test)

print(classification_report(y_test,y_pred_gbc))

skplot.metrics.plot_confusion_matrix(y_test,y_pred_gbc)

#ExtraTrees 
ExtC = ExtraTreesClassifier()


## Search grid for optimal parameters
ex_param_grid = {"max_depth": [1,2,4],
              "max_features": [1,100,1000],
              "min_samples_split": [2, 3, 10],
              "min_samples_leaf": [1, 3, 10],
              "bootstrap": [False],
              "n_estimators" :[100,300],
              "criterion": ["gini"]}


gsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring="accuracy", n_jobs= -1, verbose = 1)

gsExtC.fit(X_train,y_train)

ExtC_best = gsExtC.best_estimator_

best_classifiers.append(ExtC_best)

# Best score

ExtraTrees_score= gsExtC.best_score_

print(f' ExtraTrees score is :{ExtraTrees_score}')

y_pred_extc = gsExtC.predict(X_test)

print(classification_report(y_test,y_pred_extc))

skplot.metrics.plot_confusion_matrix(y_test,y_pred_extc)

### SVC classifier
SVMC = SVC(probability=True)
svc_param_grid = {'kernel': ['rbf'], 
                  'gamma': [ 0.001, 0.01, 0.1, 1],
                  'C': [1, 10, 50, 100,200,300, 1000]}

gsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring="accuracy", n_jobs= -1, verbose = 1)

gsSVMC.fit(X_train,y_train)

SVMC_best = gsSVMC.best_estimator_

best_classifiers.append(SVMC_best)

# Best score
SVC_score = gsSVMC.best_score_

print(f' SVC classifier score isÂ :{SVC_score}')

y_pred_svmc = gsSVMC.predict(X_test)

print(classification_report(y_test,y_pred_svmc))

skplot.metrics.plot_confusion_matrix(y_test,y_pred_svmc)

#to check accuracy
from sklearn.metrics import accuracy_score

models = ["Random Forest","Gradient Boosting Classifier","Extra Tress Classifier","Logistic Regression","SVC"]

test_accuracies = [

    accuracy_score(y_test,y_pred_rf),
    accuracy_score(y_test,y_pred_gbc),
    accuracy_score(y_test,y_pred_extc),
    accuracy_score(y_test,y_pred_lr),
    accuracy_score(y_test,y_pred_svmc)

]

accuracies = pd.DataFrame({
    
    "Model": models,
    "Accuracy Score": test_accuracies
    
})

accuracies.sort_values('Accuracy Score',ascending = False)

#Logistic Regression
model = log

model.fit(X_train, y_train)

#save the model
import pickle
filename = 'trained_model.sav'
pickle.dump(model, open(filename, 'wb'))

load_model =  pickle.load(open(filename,'rb'))

X_test.head()

load_model.predict([[7,1,1,20,1,23,0]])

input_data = (2,5,4,7000000,3,23, 1)

#change the input to numpy array
input_data_as_numpy_array = np.asarray(input_data)

#reshape the array as we predicting for one instance
input_data_reshaped =  input_data_as_numpy_array.reshape(1,-1)

prediction = load_model.predict(input_data_reshaped)
print (prediction)

if (prediction[0]==0):
  print('Startup will be unsuccessful')

else:
  print('Startup will be successful')